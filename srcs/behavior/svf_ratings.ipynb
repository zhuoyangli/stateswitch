{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0616c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root set to: /home/zli230/projects/stateswitch\n",
      "Warning: Could not import configs.config, using defaults\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "\n",
    "project_root = Path(os.getcwd()).parent\n",
    "print(f\"Project root set to: {project_root}\")\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "try:\n",
    "    from configs.config import DATA_DIR, DERIVATIVES_DIR, FIGS_DIR\n",
    "except ImportError:\n",
    "    print(\"Warning: Could not import configs.config, using defaults\")\n",
    "    DATA_DIR = Path(\"./data\")\n",
    "    DERIVATIVES_DIR = Path(\"./derivatives\")\n",
    "    FIGS_DIR = Path(\"./figs\")\n",
    "\n",
    "RATERS = {'JC', 'AS', 'GL', 'KG'}\n",
    "\n",
    "raters_dir = DATA_DIR / \"rec/svf_ratings/\"\n",
    "\n",
    "def load_ratings(file_path):\n",
    "    \"\"\"Load ratings from a xlsx file into a pandas DataFrame.\"\"\"\n",
    "    df = pd.read_excel(file_path, engine='openpyxl')\n",
    "    # Forward-fill category column to handle NaN values\n",
    "    if 'category' in df.columns:\n",
    "        df['category'] = df['category'].ffill()\n",
    "    return df\n",
    "\n",
    "def load_ratings_by_rater(rater: str) -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"Load ratings for a specific rater from corresponding folder.\"\"\"\n",
    "    if rater not in RATERS:\n",
    "        raise ValueError(f\"Rater '{rater}' is not recognized. Valid raters are: {RATERS}\")\n",
    "    rater_dir = os.path.join(raters_dir, rater)\n",
    "    xlsx_files = [f for f in os.listdir(rater_dir) if f.endswith('.xlsx')]\n",
    "    ratings_session = {}\n",
    "    for file in xlsx_files:\n",
    "        session_name = file.split('_task-svf')[0]\n",
    "        file_path = os.path.join(rater_dir, file)\n",
    "        ratings_session[session_name] = load_ratings(file_path)\n",
    "    return ratings_session\n",
    "\n",
    "def load_all_ratings() -> dict[str, dict[str, pd.DataFrame]]:\n",
    "    \"\"\"Load ratings from all raters.\"\"\"\n",
    "    all_ratings = {}\n",
    "    for rater in RATERS:\n",
    "        try:\n",
    "            all_ratings[rater] = load_ratings_by_rater(rater)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Directory for rater '{rater}' not found.\")\n",
    "    return all_ratings\n",
    "\n",
    "def get_all_sessions(all_ratings: dict) -> set:\n",
    "    \"\"\"Get all unique sessions across all raters.\"\"\"\n",
    "    sessions = set()\n",
    "    for rater, rater_sessions in all_ratings.items():\n",
    "        sessions.update(rater_sessions.keys())\n",
    "    return sessions\n",
    "\n",
    "def get_shared_sessions(all_ratings: dict, raters: list = None) -> set:\n",
    "    \"\"\"Get sessions that are rated by all specified raters.\"\"\"\n",
    "    if raters is None:\n",
    "        raters = list(all_ratings.keys())\n",
    "    \n",
    "    session_sets = [set(all_ratings[r].keys()) for r in raters if r in all_ratings]\n",
    "    if not session_sets:\n",
    "        return set()\n",
    "    \n",
    "    return set.intersection(*session_sets)\n",
    "\n",
    "def filter_for_analysis(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter DataFrame for analysis by excluding:\n",
    "    - 'next' marker rows\n",
    "    - First word of each category\n",
    "    Returns a filtered copy, does not modify original.\n",
    "    \"\"\"\n",
    "    # Exclude 'next' markers\n",
    "    mask = df['word'] != 'next'\n",
    "    \n",
    "    # Exclude first word of each category\n",
    "    is_first_in_category = df['category'] != df['category'].shift(1)\n",
    "    mask = mask & ~is_first_in_category\n",
    "    \n",
    "    return df[mask]\n",
    "\n",
    "def align_ratings_by_word(all_ratings: dict, session: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Align ratings from different raters for the same session by word.\n",
    "    Uses (category, word, start) as a unique identifier for each item.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for rater, sessions in all_ratings.items():\n",
    "        if session in sessions:\n",
    "            df = sessions[session].copy()\n",
    "            df = filter_for_analysis(df)\n",
    "            df = df.rename(columns={'switch_flag': f'switch_flag_{rater}'})\n",
    "            df = df[['category', 'word', 'start', f'switch_flag_{rater}']]\n",
    "            dfs.append(df)\n",
    "    \n",
    "    if len(dfs) < 2:\n",
    "        raise ValueError(f\"Need at least 2 raters for session '{session}', found {len(dfs)}\")\n",
    "    \n",
    "    merged = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        merged = pd.merge(merged, df, on=['category', 'word', 'start'], how='outer')\n",
    "    \n",
    "    return merged\n",
    "\n",
    "def align_ratings_all_sessions(all_ratings: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Align ratings from all raters across all sessions.\n",
    "    Returns a single DataFrame with session as an additional column.\n",
    "    \"\"\"\n",
    "    all_aligned = []\n",
    "    sessions = get_all_sessions(all_ratings)\n",
    "    \n",
    "    for session in sessions:\n",
    "        raters_with_session = [r for r in all_ratings if session in all_ratings[r]]\n",
    "        if len(raters_with_session) < 2:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            aligned = align_ratings_by_word(all_ratings, session)\n",
    "            aligned['session'] = session\n",
    "            all_aligned.append(aligned)\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping session '{session}': {e}\")\n",
    "    \n",
    "    if not all_aligned:\n",
    "        raise ValueError(\"No sessions with at least 2 raters found.\")\n",
    "    \n",
    "    return pd.concat(all_aligned, ignore_index=True)\n",
    "\n",
    "def compute_pairwise_stats_all_sessions(all_ratings: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute inter-rater reliability statistics for all rater pairs across ALL sessions.\n",
    "    \"\"\"\n",
    "    aligned = align_ratings_all_sessions(all_ratings)\n",
    "    \n",
    "    rater_cols = [col for col in aligned.columns if col.startswith('switch_flag_')]\n",
    "    raters = [col.replace('switch_flag_', '') for col in rater_cols]\n",
    "    \n",
    "    results = []\n",
    "    for r1, r2 in combinations(raters, 2):\n",
    "        col1 = f'switch_flag_{r1}'\n",
    "        col2 = f'switch_flag_{r2}'\n",
    "        \n",
    "        valid = aligned[[col1, col2]].dropna()\n",
    "        if len(valid) == 0:\n",
    "            continue\n",
    "            \n",
    "        y1 = valid[col1].astype(int)\n",
    "        y2 = valid[col2].astype(int)\n",
    "        \n",
    "        kappa = cohen_kappa_score(y1, y2)\n",
    "        agreement = (y1 == y2).mean() * 100\n",
    "        \n",
    "        results.append({\n",
    "            'rater_1': r1,\n",
    "            'rater_2': r2,\n",
    "            'n_items': len(valid),\n",
    "            'percent_agreement': round(agreement, 2),\n",
    "            'cohen_kappa': round(kappa, 3)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def compute_confusion_matrix_all_sessions(all_ratings: dict, rater1: str, rater2: str) -> tuple[np.ndarray, list]:\n",
    "    \"\"\"\n",
    "    Compute aggregated confusion matrix for two raters across all sessions.\n",
    "    \"\"\"\n",
    "    aligned = align_ratings_all_sessions(all_ratings)\n",
    "    \n",
    "    col1 = f'switch_flag_{rater1}'\n",
    "    col2 = f'switch_flag_{rater2}'\n",
    "    \n",
    "    valid = aligned[[col1, col2]].dropna()\n",
    "    y1 = valid[col1].astype(int)\n",
    "    y2 = valid[col2].astype(int)\n",
    "    \n",
    "    labels = sorted(set(y1) | set(y2))\n",
    "    cm = confusion_matrix(y1, y2, labels=labels)\n",
    "    cm = cm / np.sum(cm.sum(axis=1, keepdims=True)) * 100  # Convert to percentages\n",
    "    cm = np.round(cm).astype(int)  # Round to integers for display\n",
    "    \n",
    "    return cm, labels\n",
    "\n",
    "def plot_confusion_matrices_all_sessions(all_ratings: dict):\n",
    "    \"\"\"\n",
    "    Plot confusion matrices for all rater pairs aggregated across all sessions.\n",
    "    \"\"\"\n",
    "    aligned = align_ratings_all_sessions(all_ratings)\n",
    "    \n",
    "    rater_cols = [col for col in aligned.columns if col.startswith('switch_flag_')]\n",
    "    raters = sorted([col.replace('switch_flag_', '') for col in rater_cols])\n",
    "    pairs = list(combinations(raters, 2))\n",
    "    \n",
    "    n_pairs = len(pairs)\n",
    "    ncols = min(3, n_pairs)\n",
    "    nrows = (n_pairs + ncols - 1) // ncols\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(5 * ncols, 4 * nrows))\n",
    "    axes = np.atleast_2d(axes).flatten()\n",
    "    \n",
    "    for idx, (r1, r2) in enumerate(pairs):\n",
    "        cm, labels = compute_confusion_matrix_all_sessions(all_ratings, r1, r2)\n",
    "        labels = ['unsure', 'cluster', 'switch']\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=labels, yticklabels=labels, ax=axes[idx], cbar_kws={'label': 'Percentage (%)'})\n",
    "        axes[idx].set_xlabel(f\"{r2}'s rating\")\n",
    "        axes[idx].set_ylabel(f\"{r1}'s rating\")\n",
    "        # axes[idx].set_title(f'{r1} vs {r2}')\n",
    "    \n",
    "    for idx in range(n_pairs, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Inter-rater Confusion Matrices (All Sessions)', y=1.02)\n",
    "    return fig\n",
    "\n",
    "def compute_stats_by_session(all_ratings: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute inter-rater stats for each session separately.\n",
    "    Useful for identifying problematic sessions.\n",
    "    \"\"\"\n",
    "    sessions = get_all_sessions(all_ratings)\n",
    "    all_results = []\n",
    "    \n",
    "    for session in sessions:\n",
    "        raters_with_session = [r for r in all_ratings if session in all_ratings[r]]\n",
    "        if len(raters_with_session) < 2:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            aligned = align_ratings_by_word(all_ratings, session)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        \n",
    "        rater_cols = [col for col in aligned.columns if col.startswith('switch_flag_')]\n",
    "        raters = [col.replace('switch_flag_', '') for col in rater_cols]\n",
    "        \n",
    "        for r1, r2 in combinations(raters, 2):\n",
    "            col1 = f'switch_flag_{r1}'\n",
    "            col2 = f'switch_flag_{r2}'\n",
    "            \n",
    "            valid = aligned[[col1, col2]].dropna()\n",
    "            if len(valid) == 0:\n",
    "                continue\n",
    "            \n",
    "            y1 = valid[col1].astype(int)\n",
    "            y2 = valid[col2].astype(int)\n",
    "            \n",
    "            kappa = cohen_kappa_score(y1, y2)\n",
    "            agreement = (y1 == y2).mean() * 100\n",
    "            \n",
    "            all_results.append({\n",
    "                'session': session,\n",
    "                'rater_1': r1,\n",
    "                'rater_2': r2,\n",
    "                'n_items': len(valid),\n",
    "                'percent_agreement': round(agreement, 2),\n",
    "                'cohen_kappa': round(kappa, 3)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "def summarize_inter_rater_reliability(all_ratings: dict):\n",
    "    \"\"\"\n",
    "    Print a comprehensive summary of inter-rater reliability.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"INTER-RATER RELIABILITY SUMMARY (ALL SESSIONS)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    overall_stats = compute_pairwise_stats_all_sessions(all_ratings)\n",
    "    print(\"\\n>>> Overall Pairwise Statistics:\\n\")\n",
    "    print(overall_stats.to_string(index=False))\n",
    "    \n",
    "    avg_kappa = overall_stats['cohen_kappa'].mean()\n",
    "    avg_agreement = overall_stats['percent_agreement'].mean()\n",
    "    print(f\"\\n>>> Average Cohen's Kappa: {avg_kappa:.3f}\")\n",
    "    print(f\">>> Average Percent Agreement: {avg_agreement:.2f}%\")\n",
    "    \n",
    "    session_stats = compute_stats_by_session(all_ratings)\n",
    "    print(\"\\n>>> Per-Session Statistics:\\n\")\n",
    "    \n",
    "    session_summary = session_stats.groupby('session').agg({\n",
    "        'n_items': 'first',\n",
    "        'percent_agreement': 'mean',\n",
    "        'cohen_kappa': 'mean'\n",
    "    }).round(3)\n",
    "    print(session_summary.to_string())\n",
    "    \n",
    "    low_kappa_sessions = session_summary[session_summary['cohen_kappa'] < 0.4]\n",
    "    if not low_kappa_sessions.empty:\n",
    "        print(\"\\n>>> Sessions with low agreement (Kappa < 0.4):\")\n",
    "        print(low_kappa_sessions.to_string())\n",
    "    \n",
    "    return overall_stats, session_stats\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Usage examples:\n",
    "# =============================================================================\n",
    "# all_ratings = load_all_ratings()\n",
    "#\n",
    "# # Option 1: Four separate plots\n",
    "# fig = plot_rating_agreement_distribution(all_ratings)\n",
    "# plt.show()\n",
    "#\n",
    "# # Option 2: Side-by-side grouped bar chart (more compact)\n",
    "# fig = plot_rating_agreement_summary(all_ratings)\n",
    "# plt.show()\n",
    "#\n",
    "# # Option 3: Print summary table\n",
    "# print_rating_agreement_table(all_ratings)\n",
    "\n",
    "# Example usage:\n",
    "# all_ratings = load_all_ratings()\n",
    "# summarize_inter_rater_reliability(all_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef82fa41",
   "metadata": {},
   "outputs": [],
   "source": "def compute_class_specific_reliability(all_ratings: dict) -> pd.DataFrame:\n    \"\"\"\n    Compute per-class inter-rater reliability metrics.\n    Shows how consistently raters identify each class (0=unsure, 1=cluster, 2=switch).\n    \"\"\"\n    aligned = align_ratings_all_sessions(all_ratings)\n    \n    rater_cols = [col for col in aligned.columns if col.startswith('switch_flag_')]\n    raters = [col.replace('switch_flag_', '') for col in rater_cols]\n    \n    class_results = []\n    class_labels = {0: 'unsure', 1: 'cluster', 2: 'switch'}\n    \n    for r1, r2 in combinations(raters, 2):\n        col1 = f'switch_flag_{r1}'\n        col2 = f'switch_flag_{r2}'\n        \n        valid = aligned[[col1, col2]].dropna()\n        if len(valid) == 0:\n            continue\n            \n        y1 = valid[col1].astype(int)\n        y2 = valid[col2].astype(int)\n        \n        for label, label_name in class_labels.items():\n            # Items where at least one rater assigned this label\n            either_labeled = (y1 == label) | (y2 == label)\n            both_labeled = (y1 == label) & (y2 == label)\n            \n            n_either = either_labeled.sum()\n            n_both = both_labeled.sum()\n            \n            # Agreement when either rater uses this label\n            if n_either > 0:\n                agreement_rate = n_both / n_either * 100\n            else:\n                agreement_rate = np.nan\n            \n            # Per-rater counts\n            n_r1 = (y1 == label).sum()\n            n_r2 = (y2 == label).sum()\n            \n            class_results.append({\n                'rater_pair': f'{r1}-{r2}',\n                'label': label,\n                'label_name': label_name,\n                'n_r1': n_r1,\n                'n_r2': n_r2,\n                'n_both_agree': n_both,\n                'n_either': n_either,\n                'agreement_pct': round(agreement_rate, 1) if not np.isnan(agreement_rate) else np.nan\n            })\n    \n    return pd.DataFrame(class_results)\n\n\ndef compute_class_confusion_summary(all_ratings: dict) -> dict:\n    \"\"\"\n    Compute detailed confusion analysis: when raters disagree, what do they confuse?\n    \"\"\"\n    aligned = align_ratings_all_sessions(all_ratings)\n    \n    rater_cols = [col for col in aligned.columns if col.startswith('switch_flag_')]\n    raters = [col.replace('switch_flag_', '') for col in rater_cols]\n    \n    # Aggregate confusion across all rater pairs\n    all_confusions = []\n    \n    for r1, r2 in combinations(raters, 2):\n        col1 = f'switch_flag_{r1}'\n        col2 = f'switch_flag_{r2}'\n        \n        valid = aligned[[col1, col2]].dropna()\n        y1 = valid[col1].astype(int)\n        y2 = valid[col2].astype(int)\n        \n        # Find disagreements\n        disagree = y1 != y2\n        for idx in valid[disagree].index:\n            all_confusions.append((int(y1.loc[idx]), int(y2.loc[idx])))\n    \n    # Count confusion patterns\n    confusion_counts = {}\n    for v1, v2 in all_confusions:\n        key = tuple(sorted([v1, v2]))\n        confusion_counts[key] = confusion_counts.get(key, 0) + 1\n    \n    return confusion_counts\n\n\ndef print_class_reliability_summary(all_ratings: dict):\n    \"\"\"\n    Print a focused summary of inter-rater reliability by class.\n    \"\"\"\n    print(\"=\" * 70)\n    print(\"CLASS-SPECIFIC INTER-RATER RELIABILITY SUMMARY\")\n    print(\"=\" * 70)\n    print(\"\\nLabels: 0=unsure, 1=cluster, 2=switch\")\n    \n    # Get class-specific stats\n    class_stats = compute_class_specific_reliability(all_ratings)\n    \n    # Summarize by class across all rater pairs\n    print(\"\\n\" + \"-\" * 70)\n    print(\"PER-CLASS AGREEMENT (when either rater assigns the label)\")\n    print(\"-\" * 70)\n    \n    for label in [1, 2]:  # Focus on cluster and switch\n        label_name = {1: 'CLUSTER', 2: 'SWITCH'}[label]\n        label_data = class_stats[class_stats['label'] == label]\n        \n        print(f\"\\n>>> {label_name} (label={label}):\")\n        print(f\"    {'Rater Pair':<12} {'R1 count':>10} {'R2 count':>10} {'Both agree':>12} {'Agreement %':>12}\")\n        print(f\"    {'-'*56}\")\n        \n        for _, row in label_data.iterrows():\n            print(f\"    {row['rater_pair']:<12} {row['n_r1']:>10} {row['n_r2']:>10} {row['n_both_agree']:>12} {row['agreement_pct']:>11.1f}%\")\n        \n        # Average across pairs\n        avg_agreement = label_data['agreement_pct'].mean()\n        total_r1 = label_data['n_r1'].sum()\n        total_r2 = label_data['n_r2'].sum()\n        total_both = label_data['n_both_agree'].sum()\n        print(f\"    {'-'*56}\")\n        print(f\"    {'AVERAGE':<12} {total_r1//3:>10} {total_r2//3:>10} {total_both//3:>12} {avg_agreement:>11.1f}%\")\n    \n    # Confusion analysis\n    print(\"\\n\" + \"-\" * 70)\n    print(\"DISAGREEMENT PATTERNS (what gets confused?)\")\n    print(\"-\" * 70)\n    \n    confusion_counts = compute_class_confusion_summary(all_ratings)\n    label_names = {0: 'unsure', 1: 'cluster', 2: 'switch'}\n    \n    total_disagreements = sum(confusion_counts.values())\n    print(f\"\\nTotal disagreements across all rater pairs: {total_disagreements}\")\n    print(f\"\\nBreakdown of confusion types:\")\n    \n    for (v1, v2), count in sorted(confusion_counts.items(), key=lambda x: -x[1]):\n        pct = count / total_disagreements * 100\n        print(f\"    {label_names[v1]:>7} <-> {label_names[v2]:<7}: {count:>5} ({pct:>5.1f}%)\")\n    \n    # Key insight: cluster vs switch confusion\n    cluster_switch_confusion = confusion_counts.get((1, 2), 0)\n    print(f\"\\n>>> KEY METRIC: Cluster vs Switch confusion rate: {cluster_switch_confusion/total_disagreements*100:.1f}% of all disagreements\")\n    \n    # Per-rater label distributions\n    print(\"\\n\" + \"-\" * 70)\n    print(\"PER-RATER LABEL DISTRIBUTIONS\")\n    print(\"-\" * 70)\n    \n    aligned = align_ratings_all_sessions(all_ratings)\n    rater_cols = [col for col in aligned.columns if col.startswith('switch_flag_')]\n    \n    print(f\"\\n    {'Rater':<8} {'Unsure (0)':>12} {'Cluster (1)':>12} {'Switch (2)':>12} {'Total':>10}\")\n    print(f\"    {'-'*54}\")\n    \n    for col in sorted(rater_cols):\n        rater = col.replace('switch_flag_', '')\n        vals = aligned[col].dropna().astype(int)\n        n0 = (vals == 0).sum()\n        n1 = (vals == 1).sum()\n        n2 = (vals == 2).sum()\n        total = len(vals)\n        print(f\"    {rater:<8} {n0:>5} ({n0/total*100:>4.1f}%) {n1:>5} ({n1/total*100:>4.1f}%) {n2:>5} ({n2/total*100:>4.1f}%) {total:>10}\")\n\n\n# Run analysis\nall_ratings = load_all_ratings()\n\n# Overall summary (existing)\nsummarize_inter_rater_reliability(all_ratings)\n\n# Class-specific summary (new)\nprint(\"\\n\\n\")\nprint_class_reliability_summary(all_ratings)"
  },
  {
   "cell_type": "markdown",
   "id": "78c6ee56",
   "metadata": {},
   "source": [
    "#JC: 0-1-2: 15/59/26\n",
    "#AS: 0-1-2: 22/55/23\n",
    "#GL: 0-1-2: 10/63/27"
   ]
  },
  {
   "cell_type": "code",
   "id": "ef240vc80mb",
   "source": "def compute_agreement_table(all_ratings: dict) -> pd.DataFrame:\n    \"\"\"\n    Compute agreement table showing how many words have N raters agreeing\n    on clustering vs switching labels.\n    \n    Returns a DataFrame with counts and proportions for each agreement level (1-4 raters).\n    \"\"\"\n    aligned = align_ratings_all_sessions(all_ratings)\n    \n    rater_cols = [col for col in aligned.columns if col.startswith('switch_flag_')]\n    n_raters = len(rater_cols)\n    \n    # For each word, count votes for clustering (1) and switching (2)\n    aligned['n_cluster'] = (aligned[rater_cols] == 1).sum(axis=1)\n    aligned['n_switch'] = (aligned[rater_cols] == 2).sum(axis=1)\n    aligned['n_valid'] = aligned[rater_cols].notna().sum(axis=1)\n    \n    # Only include words where all raters provided ratings\n    aligned_complete = aligned[aligned['n_valid'] == n_raters].copy()\n    total_words = len(aligned_complete)\n    \n    print(f\"Total words with all {n_raters} raters: {total_words}\")\n    \n    # Build agreement table\n    results = []\n    \n    for n_agree in range(1, n_raters + 1):\n        # Clustering: exactly n_agree raters said clustering (1)\n        n_cluster = (aligned_complete['n_cluster'] == n_agree).sum()\n        pct_cluster = n_cluster / total_words * 100\n        \n        # Switching: exactly n_agree raters said switching (2)\n        n_switch = (aligned_complete['n_switch'] == n_agree).sum()\n        pct_switch = n_switch / total_words * 100\n        \n        results.append({\n            'n_raters_agree': n_agree,\n            'clustering_n': n_cluster,\n            'clustering_pct': round(pct_cluster, 1),\n            'switching_n': n_switch,\n            'switching_pct': round(pct_switch, 1),\n        })\n    \n    df = pd.DataFrame(results)\n    return df, total_words\n\n\ndef print_agreement_table(all_ratings: dict):\n    \"\"\"\n    Print a formatted agreement table for clustering and switching labels.\n    \"\"\"\n    df, total_words = compute_agreement_table(all_ratings)\n    \n    print(\"=\" * 70)\n    print(\"RATER AGREEMENT TABLE FOR CLUSTERING AND SWITCHING LABELS\")\n    print(\"=\" * 70)\n    print(f\"\\nTotal words analyzed: {total_words}\")\n    print(\"\\n\" + \"-\" * 70)\n    print(f\"{'# Raters':<12} {'Clustering':^25} {'Switching':^25}\")\n    print(f\"{'Agreeing':<12} {'Count':>10} {'Proportion':>12} {'Count':>10} {'Proportion':>12}\")\n    print(\"-\" * 70)\n    \n    for _, row in df.iterrows():\n        print(f\"{row['n_raters_agree']:<12} {row['clustering_n']:>10} {row['clustering_pct']:>10.1f}% {row['switching_n']:>10} {row['switching_pct']:>10.1f}%\")\n    \n    print(\"-\" * 70)\n    \n    # Summary: consensus (>=3 raters agree)\n    consensus_cluster = df[df['n_raters_agree'] >= 3]['clustering_n'].sum()\n    consensus_switch = df[df['n_raters_agree'] >= 3]['switching_n'].sum()\n    \n    print(f\"\\nConsensus (>=3 raters agree):\")\n    print(f\"  Clustering: {consensus_cluster} words ({consensus_cluster/total_words*100:.1f}%)\")\n    print(f\"  Switching:  {consensus_switch} words ({consensus_switch/total_words*100:.1f}%)\")\n    \n    # No consensus\n    no_consensus = total_words - consensus_cluster - consensus_switch\n    print(f\"  No consensus: {no_consensus} words ({no_consensus/total_words*100:.1f}%)\")\n    \n    return df\n\n\n# Load data and run the agreement table analysis\nall_ratings = load_all_ratings()\nagreement_df = print_agreement_table(all_ratings)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stateswitch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}